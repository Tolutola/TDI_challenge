{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tolut\\Anaconda3\\lib\\site-packages\\bokeh\\util\\deprecation.py:34: BokehDeprecationWarning: \n",
      "Supplying a user-defined data source AND iterable values to glyph methods is deprecated.\n",
      "\n",
      "See https://github.com/bokeh/bokeh/issues/2056 for more information.\n",
      "\n",
      "  warn(message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.models.renderers.GlyphRenderer\">GlyphRenderer</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'c2be0258-7bda-4796-a19d-ca16693d5222', <span id=\"d45afd1e-c55c-4bfc-81e1-f3df698c28ef\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">data_source&nbsp;=&nbsp;ColumnDataSource(id='8c54acdc-3c62-4666-bf11-ef6d6e5b7413', ...),</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">glyph&nbsp;=&nbsp;Patches(id='b1cd4caa-73f2-420a-8003-3e7382bb936a', ...),</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hover_glyph&nbsp;=&nbsp;None,</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">level&nbsp;=&nbsp;'glyph',</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">nonselection_glyph&nbsp;=&nbsp;Patches(id='6d6cf98b-49bc-4a0c-b264-74b0e871b1b4', ...),</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">selection_glyph&nbsp;=&nbsp;None,</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[],</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">visible&nbsp;=&nbsp;True,</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_range_name&nbsp;=&nbsp;'default',</div></div><div class=\"43c6819c-b799-4b18-a750-605e39061b7c\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_range_name&nbsp;=&nbsp;'default')</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  var expanded = false;\n",
       "  var ellipsis = document.getElementById(\"d45afd1e-c55c-4bfc-81e1-f3df698c28ef\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    var rows = document.getElementsByClassName(\"43c6819c-b799-4b18-a750-605e39061b7c\");\n",
       "    for (var i = 0; i < rows.length; i++) {\n",
       "      var el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "GlyphRenderer(id='c2be0258-7bda-4796-a19d-ca16693d5222', ...)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "192065"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "# data preprocessing\n",
    "\n",
    "\n",
    "filenames=['DS0003','DS0006','DS0009','DS0012','DS0015','DS0017']\n",
    "cohorts=np.arange(1,7).astype(str).tolist()\n",
    "\n",
    "data = {}\n",
    "data_size=0\n",
    "# fpath='C:\\\\Users\\\\tolut\\\\Box Sync\\\\Documents\\\\PythonScripts\\\\dataIncubator\\\\ICPSR_23380\\\\'\n",
    "fpath='ICPSR_23380\\\\'\n",
    "for index, file in enumerate(filenames):\n",
    "    fname = fpath+file+'\\\\23380-'+file[2:]+'-Data.tsv'\n",
    "    temp_pd=pd.read_csv(fname,sep='\\t',na_values=np.nan,dtype=str)\n",
    "    temp_pd['cohort']=cohorts[index]\n",
    "    data[cohorts[index]]=temp_pd\n",
    "    data_size+=os.stat(fname).st_size\n",
    "\n",
    "    \n",
    "data_size=data_size/1e6\n",
    "\n",
    "columnNames={}\n",
    "for c in cohorts:\n",
    "    columnNames[c]= data[c].columns\n",
    "\n",
    "#unify column names\n",
    "for index, c in enumerate(cohorts):\n",
    "    prefix1='C'+ c\n",
    "    prefix2='R'+ c\n",
    "    prefix3='P'+ c\n",
    "    st=len(prefix1)\n",
    "    temp_columnNames=[ x.replace(c,'_')  if (x.startswith(prefix1)|x.startswith(prefix2) |x.startswith(prefix3)) \\\n",
    "                      else x  for x in data[c].columns]\n",
    "    data[c].columns=temp_columnNames\n",
    "\n",
    "#accounting for changes in column names\n",
    "# change the PLREG column name in cohort 6\n",
    "data['6'].rename(columns={'P_PLREG':'PLREG'},inplace=True)\n",
    "data['2'].rename(columns={'C_DEP_YR':'C_DEP2YR'},inplace=True)\n",
    "data['2'].rename(columns={'R_DEP_YR':'R_DEP2YR'},inplace=True)\n",
    "data['1'].rename(columns={'C_SMK_00':'C_SMK100'},inplace=True)\n",
    "data['1'].rename(columns={'R_SMK_00':'R_SMK100'},inplace=True)\n",
    "data['1'].rename(columns={'C_WLK_BK':'C_WLK1BK'},inplace=True)\n",
    "data['1'].rename(columns={'R_WLK_BK':'R_WLK1BK'},inplace=True)\n",
    "data['3'].rename(columns={'C_CASE_ID':'CASE_ID'},inplace=True)\n",
    "data['4'].rename(columns={'SFLAG':'STATUS'},inplace=True)\n",
    "data['5'].rename(columns={'SFLAG':'STATUS'},inplace=True)\n",
    "data['6'].rename(columns={'SFLAG':'STATUS'},inplace=True)\n",
    "\n",
    "# combine\n",
    "com_cols=set(data['1'])\n",
    "for c in cohorts:\n",
    "    com_cols=com_cols & set(data[c])\n",
    "\n",
    "com_cols=list(com_cols)\n",
    "    \n",
    "c_data=data['1'][com_cols]\n",
    "for c in cohorts[1:]:\n",
    "    c_data=c_data.append(data[c][com_cols])\n",
    "\n",
    "# deal with missing values and tranform ordinal variables\n",
    "c_data.replace(r'\\s+', np.nan, regex=True,inplace=True)\n",
    "# data clean up and transformation (1)\n",
    "d_dict=pd.ExcelFile('insurance.xlsx').parse('Sheet1') # arbitrary scale created for data columns\n",
    "cat_dict=dict(zip(d_dict.feature,d_dict.Category))\n",
    "longName_dict=dict(zip(d_dict.feature,d_dict['long name']))\n",
    "Categories=list(set(cat_dict.values()))\n",
    "\n",
    "#create transformation dictionary\n",
    "trans_dict=defaultdict(dict)\n",
    "for feature in d_dict.feature:\n",
    "    temp_allowed = d_dict.loc[d_dict.feature==feature,'allowed']\n",
    "    temp_trans=d_dict.loc[d_dict.feature==feature,'transformed']\n",
    "    if not(pd.isnull(temp_trans.values) | pd.isnull(temp_allowed.values)):\n",
    "        keys=temp_allowed.values[0].split(',')\n",
    "        values=list(map(int,temp_trans.values[0].split(',')))\n",
    "        trans_dict[feature]=defaultdict(lambda: np.nan,zip(keys,values))\n",
    "        \n",
    "t_data=c_data.copy() # transformed data\n",
    "t_data.replace(to_replace=trans_dict,inplace=True)\n",
    "\n",
    "#group features into classes\n",
    "Computables=['Health PerceptionA','Chronic ConditionsA','Physical HealthA','Lifestyle ChoicesA','Emotional HealthA', \\\n",
    "            'Health PerceptionB','Chronic ConditionsB','Physical HealthB','Lifestyle ChoicesB','Emotional HealthB']\n",
    "Compute_dict=defaultdict(list)\n",
    "for feature in  t_data.columns:\n",
    "    Compute_dict[cat_dict[feature]].append(feature)\n",
    "    \n",
    "#append new grouping features to table\n",
    "for feature in Computables:\n",
    "    temp_cols=Compute_dict.get(feature)\n",
    "    t_data[feature]=t_data[temp_cols].mean(axis=1)\n",
    "    #create list of predictor features and output targets\n",
    "pfeatures=['PLREG','cohort']\n",
    "targets=[] # target features\n",
    "\n",
    "for c in c_data.columns:\n",
    "    if c.startswith('C_'):\n",
    "        pfeatures.append(c)\n",
    "    if c.startswith('R_'):\n",
    "        targets.append(c)\n",
    "\n",
    "\n",
    "# remove unwanted features\n",
    "notWanted=['C_WHOCMP','C_SRVDSP','C_RNDNUM','C_PCTCMP','R_SVLANG','SAMPLED', \\\n",
    "           'R_WHOCMP','R_SRVDSP','R_RNDNUM','R_PCTCMP','COHORT']\n",
    "\n",
    "pfeatures=list(set(pfeatures)-set(notWanted))\n",
    "targets=list(set(targets)-set(notWanted))\n",
    "\n",
    "#remove invalid data and rows/columns with too many NaNs\n",
    "c_data2=c_data.copy()\n",
    "c_data2.drop(c_data2[c_data2.STATUS=='3'].index,inplace=True) #invalid\n",
    "\n",
    "# surveys less than 80% complete\n",
    "c_data2.drop(c_data2[c_data2.R_SRVDSP!='M10'].index,inplace=True) \n",
    "c_data2.drop(c_data2[c_data2.C_SRVDSP!='M10'].index,inplace=True) \n",
    "\n",
    "X_data=c_data2[pfeatures]\n",
    "y_data=c_data2[targets]\n",
    "\n",
    "\n",
    "X_data=X_data.applymap(lambda x: int(x) if (isinstance(x,str)) else np.nan)\n",
    "y_data=y_data.applymap(lambda x: int(x) if (isinstance(x,str)) else np.nan)\n",
    "\n",
    "# data clean up and transformation (2)\n",
    "# from statistics import mode\n",
    "#replace missing values with median\n",
    "\n",
    "def custom_replace(x):\n",
    "    md=np.nanmedian(x)\n",
    "    return x.fillna(value=md)\n",
    "    \n",
    "X_data=X_data.apply(custom_replace, axis=0)\n",
    "y_data=y_data.apply(custom_replace, axis=0)\n",
    "# #convert categorical variables to continuous using one-hot encoding\n",
    "X_data=pd.get_dummies(X_data,columns=pfeatures)\n",
    "\n",
    "# exploratory data analyses\n",
    "#transformation dictionaries\n",
    "rdict={}\n",
    "rdict['1']='Region I - Boston'\n",
    "rdict['2']='Region II - New York'\n",
    "rdict['3']='Region III - Philadelphia'\n",
    "rdict['4']='Region IV - Atlanta'\n",
    "rdict['5']='Region V - Chicago'\n",
    "rdict['6']='Region VI - Dallas'\n",
    "rdict['7']='Region VII - Kansas City'\n",
    "rdict['8']='Region VIII - Denver'\n",
    "rdict['9']='Region IX - San Francisco'\n",
    "rdict['10']='Region X - Seattle'\n",
    "\n",
    "racedict=dict(zip(['1','2','3'],['White','Black','Other']))\n",
    "gendict=dict(zip(['1','2'],['Male','Female']))\n",
    "agedict=dict(zip(['1','2','3'],['<65','65-74','>74']))\n",
    "mrdict=dict(zip(['1','2'],['Married','Non-Married']))\n",
    "educdict=dict(zip(['1','2','3'],['< high school or GED','high school or GED','> high school or GED']))\n",
    "\n",
    "replace_dict={'PLREG':rdict,'C_RACE':racedict,'C_GENDER':gendict,'C_AGEGRP':agedict, \\\n",
    "        'C_MRSTAT':mrdict,'C_EDUC':educdict}\n",
    "\n",
    "t_data.replace(to_replace=replace_dict,inplace=True)\n",
    "\n",
    "t_data2=t_data.copy()\n",
    "#remove incomplete rows\n",
    "t_data2.drop(t_data2[t_data2.STATUS=='3'].index,inplace=True) #invalid\n",
    "# surveys less than 80% complete\n",
    "t_data2.drop(t_data2[t_data2.R_SRVDSP!='M10'].index,inplace=True) \n",
    "t_data2.drop(t_data2[t_data2.C_SRVDSP!='M10'].index,inplace=True) \n",
    "\n",
    "special=['cohort','PLREG','C_AGEGRP','C_RACE','C_GENDER','C_MRSTAT','C_EDUC','Health PerceptionA','Chronic ConditionsA',\\\n",
    "          'Physical HealthA','Lifestyle ChoicesA','Emotional HealthA', \\\n",
    "            'Health PerceptionB','Chronic ConditionsB','Physical HealthB','Lifestyle ChoicesB','Emotional HealthB']\n",
    "sp_data=pd.DataFrame()\n",
    "sp_data=sp_data.append(t_data2[special])\n",
    "\n",
    "change_labels=['Health Perception','Physical Health','Emotional Health','Chronic Conditions']\n",
    "\n",
    "for cl in change_labels:\n",
    "    temp_cl1=cl+'A'\n",
    "    temp_cl2=cl+'B'\n",
    "#     sp_data[cl]=(c_data[temp_cl2]-c_data[temp_cl1])/c_data[temp_cl1]*100\n",
    "    sp_data[cl]=sp_data[temp_cl2]/sp_data[temp_cl1]*100\n",
    "\n",
    "# sp_data.groupby('PLREG').median()\n",
    "map_data=sp_data.groupby('PLREG').median()\n",
    "#visualization I\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from bokeh.io import show,output_notebook\n",
    "from bokeh.models import (\n",
    "    ColumnDataSource,\n",
    "    HoverTool,\n",
    "    LogColorMapper\n",
    ")\n",
    "from collections import defaultdict\n",
    "from bokeh.palettes import Viridis6 as palette\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.sampledata.us_states import data as states\n",
    "\n",
    "#remove non mainland states\n",
    "del states[\"HI\"]\n",
    "del states[\"AK\"]\n",
    "\n",
    "palette.extend(palette[0:4])\n",
    "region_dict=defaultdict(list)\n",
    "rate_dict={}\n",
    "color_dict={}\n",
    "region_dict['Region I - Boston']=['CT','ME','MA','NH','RI','VT']\n",
    "region_dict['Region II - New York']=['NY','NJ','PR','VI']\n",
    "region_dict['Region III - Philadelphia']=['DE','DC','MD','PA','VA','WV']\n",
    "region_dict['Region IV - Atlanta']=['AL','FL','GA','KY','MS','NC','SC','TN']\n",
    "region_dict['Region V - Chicago']=['IL','IN','MI','MN','OH','WI']\n",
    "region_dict['Region VI - Dallas']=['AR','LA','NM','OK','TX']\n",
    "region_dict['Region VII - Kansas City']=['IA','KS','MO','NE']\n",
    "region_dict['Region VIII - Denver']=['CO','MT','ND','SD','UT','WY']\n",
    "region_dict['Region IX - San Francisco']=['AZ','CA','GU','HI','NV']\n",
    "region_dict['Region X - Seattle']=['AK','ID','OR','WA']\n",
    "\n",
    "for index, key in enumerate(region_dict.keys()):\n",
    "    rate_dict[key]=index\n",
    "    color_dict[key]=palette[index]\n",
    "\n",
    "state_xs = [states[code][\"lons\"] for code in states]\n",
    "state_ys = [states[code][\"lats\"] for code in states]\n",
    "fill_colors=[]\n",
    "region_names=[]\n",
    "region_rates=[]\n",
    "state_names=[]\n",
    "# health outcome indices\n",
    "PHA=[]\n",
    "EHA=[]\n",
    "HP=[]\n",
    "PHA_f=[]\n",
    "EHA_f=[]\n",
    "HP_f=[]\n",
    "\n",
    "for x in states.keys():\n",
    "    for key, value in region_dict.items():\n",
    "        if x in value:\n",
    "            region_names.append(key)\n",
    "            region_rates.append(rate_dict[key])\n",
    "            fill_colors.append(color_dict[key])\n",
    "            state_names.append(states[x]['name'])\n",
    "            PHA.append(np.round(map_data.loc[key,'Physical HealthA'],2))\n",
    "            EHA.append(np.round(map_data.loc[key,'Emotional HealthA'],2))\n",
    "            HP.append(np.round(map_data.loc[key,'Health PerceptionA'],2))\n",
    "            PHA_f.append(np.round(map_data.loc[key,'Physical Health'],2))\n",
    "            EHA_f.append(np.round(map_data.loc[key,'Emotional Health'],2))\n",
    "            HP_f.append(np.round(map_data.loc[key,'Health Perception'],2))\n",
    "             \n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=state_xs,\n",
    "    y=state_ys,\n",
    "    name=region_names,\n",
    "    sname=state_names,\n",
    "    rate=region_rates,\n",
    "    PHA_=PHA,\n",
    "    EHA_=EHA,\n",
    "    HP_=HP,\n",
    "    PHA_f_=PHA_f,\n",
    "    EHA_f_=EHA_f,\n",
    "    HP_f_=HP_f\n",
    "    \n",
    "))\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,reset,hover,save\"\n",
    "\n",
    "p = figure(\n",
    "    title=\"Health Outcomes survey, 1998-2014\", tools=TOOLS,\n",
    "    x_axis_location=None, y_axis_location=None,\n",
    "    plot_width=1100,plot_height=800\n",
    ")\n",
    "p.grid.grid_line_color = None\n",
    "\n",
    "p.patches('x', 'y', source=source,\n",
    "          fill_color=fill_colors,\n",
    "          fill_alpha=0.7, line_color='white', line_width=0.5)\n",
    "\n",
    "hover = p.select_one(HoverTool)\n",
    "hover.point_policy = \"follow_mouse\"\n",
    "hover.tooltips = [\n",
    "    (\"Region\", \"@name\"),\n",
    "    (\"State\", \"@sname\"),\n",
    "    (\"Physical Health\", \"@PHA_\"),\n",
    "    (\"Emotional Health\", \"@EHA_\"),\n",
    "    (\"Health Perception\", \"@HP_\"),\n",
    "    (\"Physical Health Improvement\", \"@PHA_f_\"),\n",
    "    (\"Emotional Health Improvement\", \"@EHA_f_\"),\n",
    "    (\"Health Perception Improvement\", \"@HP_f_\"),\n",
    "    (\"(Long, Lat)\", \"($x, $y)\"),\n",
    "]\n",
    "\n",
    "\n",
    "from bokeh.resources import CDN\n",
    "from bokeh.embed import file_html\n",
    "html = file_html(p, CDN, \"Figure 1\")\n",
    "Html_file= open(\"HealthMap.html\",\"w\")\n",
    "Html_file.write(html)\n",
    "Html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
